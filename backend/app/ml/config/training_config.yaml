# ML Training Configuration

# Data augmentation parameters
augmentation:
  time_shift: 0.05  # seconds
  noise_snr_db: [10, 20]  # Signal-to-noise ratio range
  pitch_shift_semitones: [-0.2, 0.2]
  speed_change: [0.95, 1.05]
  
# Model architecture
model:
  type: "resnet_bilstm"
  resnet_variant: "resnet18"
  lstm_hidden_size: 128
  lstm_layers: 2
  dropout: 0.3
  
# Training parameters
training:
  batch_size: 32
  epochs: 30
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: "adamw"
  
  # Learning rate scheduling
  scheduler:
    type: "reduce_on_plateau"
    mode: "min"
    factor: 0.5
    patience: 5
    min_lr: 0.00001
  
  # Early stopping
  early_stopping:
    patience: 5
    min_delta: 0.001
  
# Data parameters
data:
  sample_rate: 16000
  duration: 1.0  # seconds per clip
  n_mels: 64
  n_fft: 1024
  hop_length: 160
  
  # Class balancing
  balance_classes: true
  oversampling_ratio: 1.5
  
# Validation
validation:
  split_ratio: 0.15
  stratified: true
  
# Thresholds per error type
thresholds:
  madd_short:
    min_duration: 0.18  # seconds
    max_duration: 0.25
  madd_long:
    min_duration: 0.36
    max_duration: 0.5
  ghunnah:
    min_duration: 0.12
    nasal_energy_threshold: 0.6
  qalqalah:
    burst_energy_threshold: 0.7
  
# Minimum dataset sizes before training
min_samples:
  madd_short: 200
  madd_long: 200
  ghunnah_missing: 150
  qalqalah_missing: 150
  substituted_letter: 300
  
# Logging
logging:
  use_wandb: true
  wandb_project: "qari-tajweed"
  log_interval: 10  # batches
  save_best_only: true
  
# Hardware
hardware:
  use_gpu: true
  mixed_precision: true  # FP16 training
  num_workers: 4
